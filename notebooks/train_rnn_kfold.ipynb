{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99db132-16bd-4c3f-b956-49a8f76c69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-19 23:41:58.939449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-19 23:41:59.815185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../..')\n",
    "from RB_ZTF.scripts.datasets import *\n",
    "from RB_ZTF.scripts.rnn import *\n",
    "from RB_ZTF.scripts.losses import *\n",
    "\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "set_random_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555f6de8-c8c4-4284-b8fb-939869e8a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset..\n",
      "Making dataloaders..\n"
     ]
    }
   ],
   "source": [
    "print('Creating dataset..')\n",
    "oids, labels = get_only_r_oids('../akb.ztf.snad.space.json')\n",
    "data = EmbsSequenceData(oids, labels, label_type='long', path='../embeddings/')\n",
    "\n",
    "fold1, fold2, fold3, fold4, fold5 = random_split(data, [0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "folds = [fold1, fold2, fold3, fold4, fold5]\n",
    "\n",
    "print('Making dataloaders..')\n",
    "bucket_boundaries = [200, 400, 600, 800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeffe45f-f1b8-43f4-9834-482ded10cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_args = {'baseline': {'model':{'rnn_type': 'GRU'}, 'crit': nn.CrossEntropyLoss(), 'optim': {}},\n",
    "        'wd_2dir': {'model':{'rnn_type': 'GRU', 'bidirectional': True,}, 'crit': nn.CrossEntropyLoss(), 'optim': {'weight_decay': 1e-5}},\n",
    "        'tversky': {'model':{'rnn_type': 'GRU'}, 'crit': rnn_loss_handler, 'optim': {}},\n",
    "        'lstm': {'model':{'rnn_type': 'LSTM'}, 'crit': nn.CrossEntropyLoss(), 'optim': {}}\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ec62a20-70f4-4eea-a45b-6f7a7e3b1628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose model to train (baseline/wd_2dir/tversky/lstm):   wd_2dir\n"
     ]
    }
   ],
   "source": [
    "name = input('Choose model to train (baseline/wd_2dir/tversky/lstm):  ')\n",
    "args = models_args[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6845284e-25a2-4ca4-8e5e-8c08ca2b28a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model training.. folds: 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [48:42<00:00,  5.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model training.. folds: 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [47:04<00:00,  5.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model training.. folds: 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [46:25<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model training.. folds: 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [46:48<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model training.. folds: 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [47:53<00:00,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wd_2dir model successfully trained\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, fold in enumerate(folds):\n",
    "    concat_folds = ConcatDataset(folds[:k] + folds[k+1:])\n",
    "    train_sampler = BySequenceLengthSampler(concat_folds, bucket_boundaries, 32, drop_last=False, shuffle=True)\n",
    "    test_sampler = BySequenceLengthSampler(fold, bucket_boundaries, 32, drop_last=False, shuffle=False)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(concat_folds, batch_size=1, \n",
    "                        batch_sampler=train_sampler, \n",
    "                        num_workers=8,\n",
    "                        collate_fn=collate,\n",
    "                        drop_last=False, pin_memory=False)\n",
    "\n",
    "    test_loader = DataLoader(fold, batch_size=1, \n",
    "                        batch_sampler=test_sampler, \n",
    "                        num_workers=8,\n",
    "                        collate_fn=collate,\n",
    "                        drop_last=False, pin_memory=False)\n",
    "\n",
    "\n",
    "\n",
    "    model = RBclassifier(hidden_size=128, latent_dim=36, out_size=2, **args['model'])\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, **args['optim'])\n",
    "    criterion = args['crit']\n",
    "    writer = SummaryWriter(f'runs/{name}_fold{k}/')\n",
    "    print(f'{name} model training.. folds: {k+1}/{len(folds)}')\n",
    "    res = []\n",
    "    for i in tqdm(range(1, 501)):\n",
    "        res.append(\n",
    "                train_rnn(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    train_loader=train_loader,\n",
    "                    test_loader=test_loader,\n",
    "                    criterion=criterion,\n",
    "                    epoch=i,\n",
    "                    device=device,\n",
    "                    writer=writer\n",
    "                    )\n",
    "            )\n",
    "    writer.flush()\n",
    "\n",
    "    model_dir = f'../trained_models/rnn/{name}'\n",
    "    if not os.path.isdir(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    torch.save(model.state_dict(), f'{model_dir}/model{k}.zip')\n",
    "\n",
    "    np.save(f'{model_dir}/result{k}.npy', res)\n",
    "\n",
    "print(f'{name} model successfully trained\\n----------------------------------------\\n----------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5eaa9-1a62-4eea-a863-735e41a3eb07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rb-ztf",
   "language": "python",
   "name": "rb-ztf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
